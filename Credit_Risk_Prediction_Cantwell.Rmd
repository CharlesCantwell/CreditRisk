---
title: "German Credit Risk Prediction"
author: "Charles Cantwell"
#date: "Nov 17, 2022"
output: 
  pdf_document:
    keep_tex: yes  
header-includes:
  - \usepackage{color}  
urlcolor: blue  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Tree-based methods

I study the German credit data set from the UC Irvine machine learning repository. 
A set of 20 covariates (attributes) are available (7 numerical, 13 categorical) for 300 customers with bad credit risk and 700 customers with good credit risk (0 = Good, 1 = Bad). 

I aim to classify a customer as good or bad with respect to credit risk. It is worse to class a customer as good when they are bad, than it is to class a customer as bad when they are good.
 
```{r, echo=TRUE,warning=FALSE,message=FALSE, eval=TRUE,out.width='90%', fig.align='center', fig.pos='h',fig.width=8,fig.height=3}  
#read data, divide into train and test
#German.credit <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data",stringsAsFactors = TRUE)
German.credit <- read.table(file='E:\\school wd\\german.data',stringsAsFactors = TRUE)
#Was unable to knit with the website being down
# You can also find a doc file with a brief description of the German credit dataset on the web.
colnames(German.credit) = c("checkaccount", "duration", "credithistory", "purpose", 
                            "amount", "saving", "presentjob", "installmentrate", 
                            "sexstatus", "otherdebtor", "resident", "property", 
                            "age", "otherinstall", "housing", "ncredits", "job", 
                            "npeople", "telephone", "foreign", "response")
German.credit$response <- ifelse(German.credit$response==1,0,1)
German.credit$response = as.factor(German.credit$response) # 2 = bad
table(German.credit$response)
# str(German.credit) # to see factors and integers, numerics

set.seed(1234) 

n <- nrow(German.credit)
in.train <- sample(1:n,0.75*n)

train <- German.credit[in.train,]
test <- German.credit[-in.train,]
```

\textbf{I will use 4 tree methods: single classification tree, bagging, random forest, boosting}

### 1. Classification tree 

### (a) Full classification tree 


```{r, echo=TRUE,warning=FALSE,message=FALSE, eval=TRUE,out.width='70%', fig.align='center', fig.pos='h',fig.width=8,fig.height=4}  
#####################
# construct full tree
#####################
library(tree)
library(pROC)

set.seed(100) 
fulltree=tree(response~.,train,split="deviance")
summary(fulltree)
par(mfrow=c(1,2))
plot(fulltree)
text(fulltree)
# print(fulltree)
fullpred=predict(fulltree,test,type="class")
testres = table(test$response,fullpred) # confusion matrix, rows=true, columns = predictions
print(testres)
1-sum(diag(testres))/(sum(testres)) # Classification error rate
predfulltree = predict(fulltree,test, type = "vector")
testfullroc=roc(test$response == "1", predfulltree[,2])
auc(testfullroc)
par(pty="s") # "s" generates a square plotting region
plot.roc(testfullroc,xlim=c(1,0),asp=1,print.auc=TRUE)
```
 
 
### (b) Pruned classification tree


```{r, echo=TRUE,warning=FALSE,message=FALSE, eval=TRUE,out.width='90%', fig.align='center', fig.pos='h',fig.width=8,fig.height=3}  
# prune the full tree
set.seed(1234)
fullcv=cv.tree(fulltree,FUN=prune.misclass,K=5)
par(mfrow=c(1,3))

par(pty="s")
plot(fullcv$size,fullcv$dev,type="b", xlab="Terminal nodes",ylab="misclassifications")
# print(fullcv)
prunesize=fullcv$size[which.min(fullcv$dev)]
prunetree=prune.misclass(fulltree,best=prunesize) 
plot(prunetree,type="proportional")
text(prunetree,pretty=1)
predprunetree = predict(prunetree,test, type = "class")
prunetest=table(test$response,predprunetree)
print(prunetest)# rows are true; columns are predictions
1-sum(diag(prunetest))/(sum(prunetest))
predprunetree = predict(prunetree,test, type = "vector")
testpruneroc=roc(test$response == "1", predprunetree[,2])
auc(testpruneroc)
par(pty="s")
plot(testpruneroc,xlim=c(1,0),print.auc=TRUE)
```
 



It is better to grow a large tree then prune it back to obtain the best subtrees. This helps lower the test error rate. This process can be computationally demanding so there are methods that select only the most import subtrees that need pruning. For example, there is a tuning parameter that controls the trade-off between the tree's complexity and its fit to the training data.

### 2. Bagged trees


```{r, echo=TRUE,warning=FALSE,message=FALSE, eval=TRUE,out.width='90%',fig.align='center', fig.pos='h',fig.width=8,fig.height=3}  
library(randomForest)
set.seed(1234)
bag=randomForest(response~., data=German.credit,subset=in.train,
                 mtry=20,ntree=500,importance=TRUE)
bag$confusion # for training data
yhat.bag=predict(bag,newdata=test)
misclass.bag=table(test$response,yhat.bag) # rows are true; columns are predictions
print(misclass.bag)
1-sum(diag(misclass.bag))/(sum(misclass.bag)) # test error rate
predbag = predict(bag,test, type = "prob") # to AUC of ROC curves
testbagroc=roc(test$response == "1", predbag[,2])
auc(testbagroc) 

# make plots 
layout(matrix(c(1,1,1),  ncol=3, byrow = TRUE),widths = c(1,4))
par(mfrow=c(1,3))
par(pty="s")
plot.roc(testbagroc,xlim=c(1,0),print.auc=TRUE)
varImpPlot(bag,pch=20,type=1)
varImpPlot(bag,pch=20,type=2)
```



Bagging helps get rid of non-robustness and high variance by averaging. Bagging reduces variance and prevents overfitting in combination with using bootstrap sampling to train the model.

### 3. Random forest 



```{r, echo=TRUE,warning=FALSE,message=FALSE, eval=TRUE,out.width='90%', fig.align='center', fig.pos='h',fig.width=8,fig.height=3}  

set.seed(1)
#rf<-randomForest(response~., data=German.credit, subset=train, mtry=6, importance=TRUE)
#rf<- randomForest(response~., data=as.numeric(train), mtry=6,ntree=500,importance=TRUE)

rf=randomForest(response~., data=German.credit,subset=in.train,
                 mtry=5,ntree=500,importance=TRUE)
#yhat.rf = predict(rf,newdata=German.credit[-train,])
yhat.rf=predict(rf,newdata=test)
#mean((yhat.rf-test)Ë†2)
rf$confusion
1-sum(diag(rf$confusion[1:2,1:2]))/(sum(rf$confusion[1:2,1:2])) #training error rate
rf.pred <- predict(rf,newdata=test)
#misclass.rf <- table(test$injury,rf.pred)
#print(misclass.rf)
#1-sum(diag(misclass.rf))/(sum(misclass.rf)) # Test error rate
varImpPlot(rf,pch=20,main="Random Forest")

misclass.rf=table(test$response,yhat.rf) # rows are true; columns are predictions
print(misclass.rf)
1-sum(diag(misclass.rf))/(sum(misclass.rf)) # test error rate
predrf = predict(rf,test, type = "prob") # to AUC of ROC curves
testrfroc=roc(test$response == "1", predrf[,2])
auc(testrfroc) 

# make plots 
layout(matrix(c(1,1,1),  ncol=3, byrow = TRUE),widths = c(1,4))
par(mfrow=c(1,3))
par(pty="s")
plot.roc(testrfroc,xlim=c(1,0),print.auc=TRUE)
varImpPlot(rf,pch=20,type=1)
varImpPlot(rf,pch=20,type=2)

```


With bagging, the number value of the parameter `mtry` is the same as the number of covariates. This parameter indicates the number of predictors used by the model. The benefit of choosing `mtry` to be a value less than the number of covariates is that it helps reduce correlated trees. This is because in bagging, if there is a strong predictor in the dataset, the decision trees produced by each of the bootstrap samples in the bagging algorithm becomes very similar. So in order to simply the model and reduce the number of highly correlated trees, the value of `mtry` is divided by three (for regression) or reduced to the square root of the original `mtry` (for classification).

### 4. Boosting


```{r, echo=TRUE,warning=FALSE,message=FALSE, eval=TRUE,out.width='50%',  fig.pos='h',fig.width=8,fig.height=5}  
#######################
# Boosting
#######################
library(gbm)
set.seed(1234)

train$response <- as.character(train$response)
boost=gbm(formula= response ~ ., distribution="bernoulli",
          data=train, n.trees=8000,shrinkage=0.001)
summary(boost,plot=FALSE)

par(pty="s")
par(mfrow=c(1,4)) 
plot(boost,i="purpose")
plot(boost,i="amount")
plot(boost,i="checkaccount") 
plot(boost,i="credithistory") 
```


```{r, echo=TRUE,warning=FALSE,message=FALSE, eval=TRUE,out.width='65%',  fig.pos='h',fig.width=8,fig.height=5}  

library(gbm) 
# make predictions
test$response <- as.character(test$response)
yhat.boost=predict(boost,newdata=test,n.trees=8000,type="response") 
boost.pred <- ifelse(yhat.boost>=0.5,1,0)

misclass.boost <- table(test$response,boost.pred)
print(misclass.boost)
1-sum(diag(misclass.boost))/(sum(misclass.boost))

testrfroc=roc(test$response == "1",yhat.boost)
auc(testrfroc) 
par(pty="s")
plot(testrfroc,xlim=c(1,0),print.auc=TRUE) 
```


For bagging, the training subsets of the data are drawn randomly replacing the training dataset. In boosting, every new subset is composed with elements
that were misclassified in previous models.


An AUC value of 1 means that the model has 100% accuracy with every positive being a true positive and every negative being a true negative. The goal for a model is to have an AUC value of 1 on every test set. The values range from zero to one. I will also discuss misclassification error rates. The classication error rate for the full classification tree method is 0.3 with an AUC value of 0.6808. After pruning the classification tree, the classification error rate increased to 0.32 with an AUC value of 0.6566. As for bagging, the test error rate is 0.284 with an AUC value of 0.7561. For the random forest method, the test error rate is 0.26 with an AUC value of 0.7639. Finally for boosting the test error rate is 0.296 with an AUC value of 0.7493. Given these results it seems the random forest model performs the best for the German credit data.
As for the models themselves, the classification tree had the lowest error at 5 terminal nodes. Additionally, the MeanDecreaseAccuracy and MeanDecreaseGini graphs seem to show a set of points that seem to lie along a logarithmic curve.

